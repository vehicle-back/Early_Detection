{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation of Models on Incident Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\haowu\\Anaconda3\\envs\\traffic\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "c:\\Users\\haowu\\Anaconda3\\envs\\traffic\\lib\\site-packages\\torchvision\\io\\image.py:13: UserWarning: Failed to load image Python extension: '[WinError 127] The specified procedure could not be found'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?\n",
      "  warn(\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import pandas as pd\n",
    "from torchmetrics import Accuracy\n",
    "# from torchmetrics.functional import precision_recall\n",
    "from tqdm import tqdm\n",
    "from datetime import datetime as dt\n",
    "from datetime import time\n",
    "from matplotlib import pyplot as plt\n",
    "from copy import deepcopy\n",
    "from torch.utils.data import DataLoader, Dataset, Subset\n",
    "from GAN_GBM import create_parser\n",
    "import matplotlib.dates as mdates\n",
    "from datetime import timedelta\n",
    "from scipy.stats import gumbel_r\n",
    "from matplotlib.dates import DateFormatter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "589.5362227805696"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "((3000000000/6368)*108.12)/(3600*24)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'LLM (Python 3.12.4)' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: 'conda install -n LLM ipykernel --update-deps --force-reinstall'"
     ]
    }
   ],
   "source": [
    "from html2text import HTML2Text\n",
    "\n",
    "def html_to_text(html: str) -> str:\n",
    "    \"\"\"Converts HTML content to plain text.\n",
    "    Args:\n",
    "        html (bytes): HTML content as bytes.\n",
    "    Returns:\n",
    "        str: Plain text extracted from HTML.\n",
    "    \"\"\"\n",
    "    html = html.decode('latin1')\n",
    "    h = HTML2Text()\n",
    "    h.ignore_links = False\n",
    "    text = h.handle(html)\n",
    "    return text\n",
    "\n",
    "# Example HTML content to run through the function\n",
    "html_example = b\"\"\"\n",
    "<html>\n",
    "<head>\n",
    "    <title>Sample HTML</title>\n",
    "</head>\n",
    "<body>\n",
    "    <h1>Main Title</h1>\n",
    "    <h2>Subtitle</h2>\n",
    "    <p>This is a paragraph with <a href=\"https://example.com\">a link</a> and an image below.</p>\n",
    "    <img src=\"https://example.com/image.png\" alt=\"Example Image\">\n",
    "    <table>\n",
    "        <tr><th>Header 1</th><th>Header 2</th></tr>\n",
    "        <tr><td>Cell 1</td><td>Cell 2</td></tr>\n",
    "    </table>\n",
    "</body>\n",
    "</html>\n",
    "\"\"\"\n",
    "\n",
    "# Run the function on the example HTML content\n",
    "plain_text = html_to_text(html_example)\n",
    "print(plain_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0. Load Args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = create_parser()\n",
    "args = parser.parse_args(args=[])\n",
    "# For reproducibility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Namespace(model_type=None, dropout_prob=0.3, seq_len_in=7, seq_len_out=6, freq_out=5, inc_threshold=0.5, LR_pos_weight=100, nonrec_spd_weight=15, use_expectation=False, use_gt_inc=0, training_granularity=1, time_series_cv_ratio=1, data_train_ratio=0.7, data_val_ratio=0.2, seed=3407, dim_in=0, dim_out=1, num_node=207, use_spd_all=True, use_spd_truck=False, use_spd_pv=False, use_slowdown_spd=True, use_tti=True, use_HA=False, use_dens=False, use_weather=False, use_time=False, use_waze=False, county='TSMO', link_id='110+04483', number_of_point_per_day=186, number_of_business_day=260, upstream_range_mile=2, downstream_range_mile=1, inc_ahead_label_min=0, task=None, num_epochs=200, batch_size=64, num_workers=0, lr=2.5e-05, nz=100, beta1=0.9, beta2=0.99, exp_name=None, data_dir='E:/two_stage_model', log_dir='./logs', checkpoint_dir='./checkpoints', checkpoint_every=100, load_checkpoint='', load_checkpoint_epoch=-1)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "args "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available() # Check if GPU is used"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_SVM_pred(use_GAN, args):\n",
    "    if use_GAN:\n",
    "        pred = np.load(f'{args.data_dir}/model/GAN_baseline/{args.county}/{args.link_id}/svm_gan_inc.npy')\n",
    "    else:\n",
    "        pred = np.load(f'{args.data_dir}/model/GAN_baseline/{args.county}/{args.link_id}/svm_inc.npy')\n",
    "    if args.county == 'TSMO':\n",
    "        pred = pred.reshape(26, 174)\n",
    "    else:\n",
    "        pred = pred.reshape(51, 174)\n",
    "    return pred\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_test_annomaly(args):\n",
    "    df_select_inc = pd.read_pickle(f\"{args.data_dir}/data/{args.county}/processed_data/{args.county}_selected_incident.pkl\")\n",
    "    link_annomaly = np.array(df_select_inc[args.link_id])\n",
    "    if args.county == 'TSMO':\n",
    "        link_annomaly = link_annomaly.reshape(260, 186)\n",
    "        test_annomaly = link_annomaly[-26:, 7:-5]\n",
    "    else:\n",
    "        link_annomaly = link_annomaly.reshape(522, 186)\n",
    "        test_annomaly = link_annomaly[-51:, 7:-5]\n",
    "    return test_annomaly"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Load Spd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_speed_for_time_range(args, plot_name, link_id, start_time, end_time, ahead_number, next_number, shadow, red_hatch, compare_days):\n",
    "    speed_df = pd.read_pickle(f\"{args.data_dir}/data/{args.county}/processed_data/{args.county}_df_spd_tmc_5min_all_from_1_min.pkl\")\n",
    "\n",
    "    \n",
    "    speed_df.reset_index(inplace=True)\n",
    "    speed_df.rename(columns={'index': 'time'}, inplace=True)\n",
    "    speed_df['time'] = pd.to_datetime(speed_df['measurement_tstamp'])\n",
    "    link_df = speed_df[[link_id, 'time']]\n",
    "\n",
    "    # 找到给定时间段的索引\n",
    "    closest_start_idx = (link_df['time'] - start_time).abs().argsort().iloc[0]\n",
    "    closest_end_idx = (link_df['time'] - end_time).abs().argsort().iloc[0]\n",
    "\n",
    "    # 向上取两个最近的五分钟\n",
    "    start_idx = max(closest_start_idx - ahead_number, 0)\n",
    "    end_idx = min(closest_end_idx + next_number, len(link_df) - 1)\n",
    "    \n",
    "    # 取出对应的时间段\n",
    "    plot_df = link_df.iloc[start_idx:end_idx+1]\n",
    "\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.plot(plot_df['time'], plot_df[link_id], marker='o', label=f'{start_time.date()} Speed')\n",
    "    plt.title(f\"{plot_name}\")\n",
    "    plt.xlabel('Time')\n",
    "    plt.ylabel('Speed')\n",
    "    plt.grid(True)\n",
    "    \n",
    "    if shadow:\n",
    "        plt.axvspan(shadow[0], shadow[1], color='red', alpha=0.3,  label=f'{start_time.date()} Incident Report')\n",
    "\n",
    "    if red_hatch:\n",
    "        plt.fill_betweenx(y=[plot_df[link_id].min(), plot_df[link_id].max()], x1=red_hatch[0], x2=red_hatch[1],\n",
    "                          edgecolor='red', facecolor='none', alpha=0.3, hatch='//', label=f'{start_time.date()} Alert')\n",
    "\n",
    "    ax = plt.gca()\n",
    "    ax.xaxis.set_major_formatter(DateFormatter('%H:%M'))  # 仅显示时刻和分钟\n",
    "\n",
    "    # 添加比较的折线图\n",
    "    for days in compare_days:\n",
    "        compare_start_time = start_time + timedelta(days=days)\n",
    "        compare_end_time = end_time + timedelta(days=days)\n",
    "        print(compare_start_time, compare_end_time)\n",
    "        print(compare_start_time.day)\n",
    "        \n",
    "        compare_closest_start_idx = (link_df['time'] - compare_start_time).abs().argsort().iloc[0]\n",
    "        compare_closest_end_idx = (link_df['time'] - compare_end_time).abs().argsort().iloc[0]\n",
    "\n",
    "        compare_start_idx = max(compare_closest_start_idx - ahead_number, 0)\n",
    "        compare_end_idx = min(compare_closest_end_idx + next_number, len(link_df) - 1)\n",
    "        \n",
    "        compare_plot_df = link_df.iloc[compare_start_idx:compare_end_idx+1]\n",
    "        plt.plot(plot_df['time'], compare_plot_df[link_id],  marker='x', linestyle='--', label=f'{compare_start_time.date()} Speed')\n",
    "        \n",
    "        # 规范化时间\n",
    "        #compare_plot_df['norm_time'] = (compare_plot_df['time'] - compare_plot_df['time'].iloc[0]).dt.total_seconds() / 60\n",
    "\n",
    "        #plt.plot(compare_plot_df['norm_time'], compare_plot_df[link_id], marker='x', linestyle='--', label=f'Comparison {days} days')\n",
    "    \n",
    "    plt.ylim(10, 75)\n",
    "\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Incident Evaluation Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "class IncidentDetectionAnalysis:\n",
    "    \n",
    "    def __init__(self, \n",
    "            args,\n",
    "            prediciotn_array, \n",
    "            annomaly_array,\n",
    "            interval =  timedelta(minutes=5), \n",
    "            prediction_start_time =  '06:00', #'06:05',\n",
    "        ):\n",
    "        self.link_id = args.link_id # tmc id link\n",
    "        self.interval = interval # every 5 minute\n",
    "        self.prediction_start_time = prediction_start_time # 6:00 a.m., str\n",
    "        self.prediction_array = prediciotn_array # model output, flattened, [number_of_days, 174]\n",
    "        self.annomaly_array = annomaly_array # denoised incident label, falttened, [number_of_days, 17?]\n",
    "        \n",
    "        df_waze = pd.read_pickle(f\"{args.data_dir}/data/{args.county}/processed_data/{args.county}_raw_waze_inc_eval.pkl\")\n",
    "\n",
    "        df_waze_link = df_waze[df_waze.id_tmc == self.link_id]\n",
    "        df_waze_link['Start time'] = pd.to_datetime(df_waze_link['Start time'], errors='coerce')\n",
    "        df_waze_link['Closed time'] = pd.to_datetime(df_waze_link['Closed time'], errors='coerce')\n",
    "        try:\n",
    "            df_waze_link['Start time'] = df_waze_link['Start time'].apply(lambda x: x.tz_localize(None))\n",
    "            df_waze_link['Closed time'] = df_waze_link['Closed time'].apply(lambda x: x.tz_localize(None))\n",
    "        except Exception as e:\n",
    "            print(f\"Error localizing time: {e}\")\n",
    "\n",
    "        self.df_waze_link = df_waze_link\n",
    "\n",
    "        self.day_n = prediciotn_array.shape[0]\n",
    "        print('day_number_is', self.day_n)\n",
    "        busi_date = pd.bdate_range(start=args.start_date, end=args.end_date).date\n",
    "        self.test_date = busi_date[-self.day_n:]\n",
    "    \n",
    "    def _get_time_periods(self, day_array, date_str):\n",
    "        prediction_start_date_time = dt.strptime(f'{date_str} {self.prediction_start_time}', '%Y-%m-%d %H:%M')\n",
    "        periods = []\n",
    "        n = len(day_array)\n",
    "        i = 0\n",
    "        while i < n:\n",
    "            if day_array[i] == 1:\n",
    "                start_idx = i\n",
    "                while i < n and day_array[i] == 1:\n",
    "                    i += 1\n",
    "                end_idx = i - 1\n",
    "                start_period = prediction_start_date_time + start_idx * self.interval\n",
    "                end_period = prediction_start_date_time + (end_idx + 1) * self.interval\n",
    "                periods.append((start_period, end_period))\n",
    "            i += 1\n",
    "        return periods\n",
    "    \n",
    "    def _generate_day_incident_periods(self, date_str):\n",
    "        \n",
    "        given_date = dt.strptime(date_str, '%Y-%m-%d')\n",
    "        start_time = given_date.replace(hour=6, minute=0, second=0) # edit as needed\n",
    "        end_time = given_date.replace(hour=20, minute=25, second=0) # edit as needed\n",
    "        df_filtered = self.df_waze_link[(self.df_waze_link['Start time'].dt.date == given_date.date()) | (self.df_waze_link['Closed time'].dt.date == given_date.date())]\n",
    "        incident_periods = []\n",
    "        \n",
    "        for _, row in df_filtered.iterrows():\n",
    "            incident_start = max(row['Start time'], start_time)\n",
    "            incident_end = min(row['Closed time'], end_time)\n",
    "            if incident_start < incident_end:\n",
    "                incident_periods.append((incident_start, incident_end))\n",
    "        \n",
    "        if not incident_periods:\n",
    "            return []\n",
    "        \n",
    "        incident_periods.sort()\n",
    "        merged_periods = []\n",
    "        current_start, current_end = incident_periods[0]\n",
    "        for start, end in incident_periods[1:]:\n",
    "            if start <= current_end:\n",
    "                current_end = max(current_end, end)\n",
    "            else:\n",
    "                merged_periods.append((current_start, current_end))\n",
    "                current_start, current_end = start, end\n",
    "        \n",
    "        merged_periods.append((current_start, current_end))\n",
    "        merged_periods = [(pd.Timestamp(start).to_pydatetime(), pd.Timestamp(end).to_pydatetime()) for start, end in merged_periods]\n",
    "        return merged_periods\n",
    "    \n",
    "    def _is_overlap(self, period1, period2):\n",
    "        return period1[0] < period2[1] and period1[1] > period2[0]\n",
    "    \n",
    "    \n",
    "    def _check_far_fp(self,  report_period, alert_period):\n",
    "        alert_number = len(alert_period)\n",
    "        false_postive_case = []\n",
    "        false_alert_number = 0\n",
    "        for alert in alert_period:\n",
    "            true_alert = False\n",
    "            for report in report_period:\n",
    "                if self._is_overlap(alert, report):\n",
    "                    true_alert = True\n",
    "            if not true_alert:\n",
    "                false_alert_number += 1\n",
    "                false_postive_case.append(alert)\n",
    "        return alert_number, false_alert_number, false_postive_case\n",
    "    \n",
    "\n",
    "    def _check_sfar_sfp(self, report_period, alert_period, annomaly_period):\n",
    "        alert_number = len(alert_period)\n",
    "        sfalse_postive_case = []\n",
    "        sfalse_alert_number = 0\n",
    "        for alert in alert_period:\n",
    "            true_alert = False\n",
    "            for report in report_period:\n",
    "                if self._is_overlap(alert, report):\n",
    "                    print('report', alert, report)\n",
    "                    true_alert = True\n",
    "            for annomaly in  annomaly_period:\n",
    "                if self._is_overlap(alert, annomaly):\n",
    "                    print('annomaly', alert, annomaly)\n",
    "                    true_alert = True\n",
    "            if not true_alert:\n",
    "                sfalse_alert_number += 1\n",
    "                sfalse_postive_case.append(alert)\n",
    "        return alert_number, sfalse_alert_number, sfalse_postive_case\n",
    "    \n",
    "    def _check_dr_mttd_fn(self, report_period, alert_period):\n",
    "\n",
    "        report_number = len(report_period)\n",
    "        total_ttd = 0\n",
    "        detected_number = 0\n",
    "        fail_to_detect_case = []\n",
    "        \n",
    "        Detected_Report_Alert_Dict = {} # key, report_id; value, alerts\n",
    "        Detected_Report_Dict = {}\n",
    "        \n",
    "        for report_id in range(len(report_period)):\n",
    "            report = report_period[report_id]\n",
    "            detected = False\n",
    "            ttd_list = []\n",
    "            for alert in alert_period:\n",
    "                if self._is_overlap(alert, report):\n",
    "\n",
    "                    if report_id in Detected_Report_Alert_Dict.keys():\n",
    "                        Detected_Report_Alert_Dict[report_id].append(alert)\n",
    "                    else:\n",
    "                        Detected_Report_Alert_Dict[report_id] = []\n",
    "                        Detected_Report_Alert_Dict[report_id] = [alert]\n",
    "                        Detected_Report_Dict[report_id] = []\n",
    "                        Detected_Report_Dict[report_id] = [report]\n",
    "\n",
    "                    detected = True\n",
    "                    time_difference = alert[0] - report[0]\n",
    "                    time_difference_in_minute = time_difference.total_seconds() / 60\n",
    "                    ttd_list.append(time_difference_in_minute)\n",
    "            \n",
    "            if detected:\n",
    "                ttd = min(ttd_list)\n",
    "                print('detected!!!!!!!', ttd_list, ttd)\n",
    "                total_ttd += ttd\n",
    "                detected_number += 1\n",
    "\n",
    "            else:\n",
    "                fail_to_detect_case.append(report)\n",
    "        \n",
    "        if detected_number != len(list(Detected_Report_Dict.keys())):\n",
    "            raise ValueError('Check _check_dr_mttd_fn Function')\n",
    "\n",
    "        return report_number, detected_number, total_ttd, fail_to_detect_case, Detected_Report_Dict, Detected_Report_Alert_Dict\n",
    "    \n",
    "    def _check_sdr_smttd_sfn(self, report_period, alert_period, annomaly_period):\n",
    "        signficant_report_period = []\n",
    "        for report in report_period:\n",
    "            for annomaly in annomaly_period:\n",
    "                if self._is_overlap(report, annomaly):\n",
    "                    signficant_report_period.append(report)\n",
    "                    break \n",
    "\n",
    "        s_report_number = len(signficant_report_period)\n",
    "        s_total_ttd = 0\n",
    "        s_detected_number = 0\n",
    "        s_fail_to_detect_case = []\n",
    "\n",
    "        Detected_Report_Alert_Dict = {} # key, report_id; value, alerts\n",
    "        Detected_Report_Dict = {}\n",
    "\n",
    "        for report_id in range(len(signficant_report_period)):\n",
    "\n",
    "            report = signficant_report_period[report_id]\n",
    "\n",
    "            detected = False\n",
    "            ttd_list = []\n",
    "            for alert in alert_period:\n",
    "                if self._is_overlap(alert, report):\n",
    "\n",
    "                    if report_id in Detected_Report_Alert_Dict.keys():\n",
    "                        Detected_Report_Alert_Dict[report_id].append(alert)\n",
    "                    else:\n",
    "                        Detected_Report_Alert_Dict[report_id] = []\n",
    "                        Detected_Report_Alert_Dict[report_id] = [alert]\n",
    "                        Detected_Report_Dict[report_id] = []\n",
    "                        Detected_Report_Dict[report_id] = [report]\n",
    "\n",
    "                    detected = True\n",
    "                    time_difference = alert[0] - report[0]\n",
    "                    time_difference_in_minute = time_difference.total_seconds() / 60\n",
    "                    ttd_list.append(time_difference_in_minute)\n",
    "            if detected:\n",
    "                ttd = min(ttd_list)\n",
    "                s_total_ttd += ttd\n",
    "                s_detected_number += 1\n",
    "            else:\n",
    "                s_fail_to_detect_case.append(report)\n",
    "            \n",
    "        if s_detected_number != len(list(Detected_Report_Dict.keys())):\n",
    "            raise ValueError('Check _check_sdr_smttd_sfn Function')\n",
    "        \n",
    "        return s_report_number, s_detected_number, s_total_ttd, s_fail_to_detect_case, Detected_Report_Dict, Detected_Report_Alert_Dict\n",
    "    \n",
    "    def check_day_result(self, day_pred_array, date_str):\n",
    "        day_report_merged_period = self._generate_day_incident_periods(date_str)\n",
    "        day_alart_period = self._get_time_periods(day_pred_array, date_str)\n",
    "        day_report_number, day_detected_number, day_total_ttd, day_fail_to_detect_cases, Day_Detected_Report_Dict, Day_Detected_Report_Alert_Dict = self._check_dr_mttd_fn(day_report_merged_period, day_alart_period)\n",
    "        day_alert_number, day_false_alert_number, day_false_postive_cases = self._check_far_fp(day_report_merged_period, day_alart_period)\n",
    "        return day_report_number, day_detected_number, day_total_ttd, day_alert_number, day_false_alert_number, day_fail_to_detect_cases, day_false_postive_cases, Day_Detected_Report_Dict, Day_Detected_Report_Alert_Dict\n",
    "    \n",
    "    def check_day_result_s(self, day_pred_array, day_annomlay_array, date_str):\n",
    "        day_report_merged_period = self._generate_day_incident_periods(date_str)\n",
    "        day_alart_period = self._get_time_periods(day_pred_array, date_str)\n",
    "        day_annom_period = self._get_time_periods(day_annomlay_array, date_str)\n",
    "        day_report_number, day_detected_number, day_total_ttd, day_fail_to_detect_cases, Day_Detected_Report_Dict, Day_Detected_Report_Alert_Dict = self._check_sdr_smttd_sfn(day_report_merged_period, day_alart_period, day_annom_period)\n",
    "        day_alert_number, day_false_alert_number, day_false_postive_cases = self._check_sfar_sfp(day_report_merged_period, day_alart_period, day_annom_period)\n",
    "        return day_report_number, day_detected_number, day_total_ttd, day_alert_number, day_false_alert_number, day_fail_to_detect_cases, day_false_postive_cases, Day_Detected_Report_Dict, Day_Detected_Report_Alert_Dict\n",
    "    \n",
    "    def check_overall_results(self):\n",
    "        total_report_number, total_detected_number, total_ttd, total_alert_number, total_false_alert_number = 0, 0, 0, 0, 0\n",
    "        all_fail_to_detect_cases = []\n",
    "        all_false_postive_cases = []\n",
    "\n",
    "        Detected_Report_Dict = {}\n",
    "        Detected_Report_Alert_Dict = {}\n",
    "\n",
    "        for i in range(len(self.test_date)):\n",
    "            day_pred_array = self.prediction_array[i]\n",
    "            date_str = self.test_date[i].strftime(\"%Y-%m-%d\")\n",
    "            day_report_number, day_detected_number, day_total_ttd, day_alert_number, day_false_alert_number, day_fail_to_detect_cases, day_false_postive_cases, Day_Detected_Report_Dict, Day_Detected_Report_Alert_Dict = self.check_day_result(day_pred_array, date_str)\n",
    "            total_report_number += day_report_number\n",
    "            total_detected_number += day_detected_number\n",
    "            total_ttd += day_total_ttd \n",
    "            total_alert_number += day_alert_number \n",
    "            total_false_alert_number += day_false_alert_number\n",
    "            all_fail_to_detect_cases.extend(day_fail_to_detect_cases)\n",
    "            all_false_postive_cases.extend(day_false_postive_cases)\n",
    "\n",
    "            report_id_list = list(Day_Detected_Report_Dict.keys())\n",
    "\n",
    "            for j in range(day_detected_number):\n",
    "                Detected_Report_Dict[total_detected_number-day_detected_number+j] = []\n",
    "                Detected_Report_Alert_Dict[total_detected_number-day_detected_number+j] = []\n",
    "                report_id = report_id_list[j]\n",
    "                Detected_Report_Dict[total_detected_number-day_detected_number+j] = Day_Detected_Report_Dict[report_id]\n",
    "                Detected_Report_Alert_Dict[total_detected_number-day_detected_number+j] = Day_Detected_Report_Alert_Dict[report_id]\n",
    "        \n",
    "        print('total_detected_number', total_detected_number)\n",
    "        print('total_report_number', total_report_number)\n",
    "        if total_report_number != 0:\n",
    "            DR = total_detected_number/total_report_number\n",
    "        else:\n",
    "            DR = 0\n",
    "        \n",
    "        if total_detected_number !=0:\n",
    "            MTTD = total_ttd/total_detected_number\n",
    "        else:\n",
    "            MTTD = 0\n",
    "\n",
    "        if total_alert_number!= 0:\n",
    "            FAR = total_false_alert_number/total_alert_number\n",
    "        else:\n",
    "            FAR = 0\n",
    "        \n",
    "        return DR, FAR, MTTD, all_fail_to_detect_cases, all_false_postive_cases, Detected_Report_Dict, Detected_Report_Alert_Dict\n",
    "    \n",
    "    def check_overall_s_results(self):\n",
    "        total_report_number, total_detected_number, total_ttd, total_alert_number, total_false_alert_number = 0, 0, 0, 0, 0\n",
    "        all_fail_to_detect_cases = []\n",
    "        all_false_postive_cases = []\n",
    "\n",
    "        Detected_Report_Dict = {}\n",
    "        Detected_Report_Alert_Dict = {}\n",
    "\n",
    "        for i in range(len(self.test_date)):\n",
    "            day_pred_array = self.prediction_array[i]\n",
    "            day_anno_array = self.annomaly_array[i]\n",
    "            date_str = self.test_date[i].strftime(\"%Y-%m-%d\")\n",
    "            day_report_number, day_detected_number, day_total_ttd, day_alert_number, day_false_alert_number, day_fail_to_detect_cases, day_false_postive_cases, Day_Detected_Report_Dict, Day_Detected_Report_Alert_Dict = self.check_day_result_s(day_pred_array, day_anno_array, date_str)\n",
    "            total_report_number += day_report_number\n",
    "            total_detected_number += day_detected_number\n",
    "            total_ttd += day_total_ttd \n",
    "            total_alert_number += day_alert_number \n",
    "            total_false_alert_number += day_false_alert_number\n",
    "            all_fail_to_detect_cases.extend(day_fail_to_detect_cases)\n",
    "            all_false_postive_cases.extend(day_false_postive_cases)\n",
    "\n",
    "\n",
    "            report_id_list = list(Day_Detected_Report_Dict.keys())\n",
    "\n",
    "            for j in range(day_detected_number):\n",
    "                Detected_Report_Dict[total_detected_number-day_detected_number+j] = []\n",
    "                Detected_Report_Alert_Dict[total_detected_number-day_detected_number+j] = []\n",
    "                report_id = report_id_list[j]\n",
    "                Detected_Report_Dict[total_detected_number-day_detected_number+j] = Day_Detected_Report_Dict[report_id]\n",
    "                Detected_Report_Alert_Dict[total_detected_number-day_detected_number+j] = Day_Detected_Report_Alert_Dict[report_id]\n",
    "        \n",
    "        print('total_detected_number', total_detected_number)\n",
    "        print('total_report_number', total_report_number)\n",
    "        if total_report_number != 0:\n",
    "            DR = total_detected_number/total_report_number\n",
    "        else:\n",
    "            DR = 0\n",
    "        \n",
    "        if total_detected_number !=0:\n",
    "            MTTD = total_ttd/total_detected_number\n",
    "        else:\n",
    "            MTTD = 0\n",
    "\n",
    "        if total_alert_number!= 0:\n",
    "            FAR = total_false_alert_number/total_alert_number\n",
    "        else:\n",
    "            print('no alert the whole day!!!!!!!!!')\n",
    "            FAR = 0\n",
    "        \n",
    "        return DR, FAR, MTTD, all_fail_to_detect_cases, all_false_postive_cases, Detected_Report_Dict, Detected_Report_Alert_Dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "day_number_is 26\n",
      "detected!!!!!!! [-8.166666666666666] -8.166666666666666\n",
      "detected!!!!!!! [-66.38333333333334] -66.38333333333334\n",
      "detected!!!!!!! [-17.566666666666666] -17.566666666666666\n",
      "detected!!!!!!! [-8.183333333333334] -8.183333333333334\n",
      "detected!!!!!!! [-65.13333333333334] -65.13333333333334\n",
      "DR 0.7142857142857143 FAR 0.975609756097561 MTTD -33.086666666666666\n",
      "SDR 1.0 SFAR 0.9451219512195121 SMTTD -24.762500000000003\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\haowu\\AppData\\Local\\Temp\\ipykernel_512216\\3925642866.py:19: FutureWarning: In a future version of pandas, parsing datetimes with mixed time zones will raise an error unless `utc=True`. Please specify `utc=True` to opt in to the new behaviour and silence this warning. To create a `Series` with mixed offsets and `object` dtype, please use `apply` and `datetime.datetime.strptime`\n",
      "  df_waze_link['Start time'] = pd.to_datetime(df_waze_link['Start time'], errors='coerce')\n",
      "c:\\Users\\haowu\\Anaconda3\\envs\\traffic\\lib\\site-packages\\geopandas\\geodataframe.py:1443: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  super().__setitem__(key, value)\n",
      "C:\\Users\\haowu\\AppData\\Local\\Temp\\ipykernel_512216\\3925642866.py:20: FutureWarning: In a future version of pandas, parsing datetimes with mixed time zones will raise an error unless `utc=True`. Please specify `utc=True` to opt in to the new behaviour and silence this warning. To create a `Series` with mixed offsets and `object` dtype, please use `apply` and `datetime.datetime.strptime`\n",
      "  df_waze_link['Closed time'] = pd.to_datetime(df_waze_link['Closed time'], errors='coerce')\n",
      "c:\\Users\\haowu\\Anaconda3\\envs\\traffic\\lib\\site-packages\\geopandas\\geodataframe.py:1443: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  super().__setitem__(key, value)\n"
     ]
    }
   ],
   "source": [
    "args.link_id = '110+04483'\n",
    "args.county = \"TSMO\"\n",
    "args.model_path = 'E:/two_stage_model'\n",
    "args.start_date = dt(2022, 2, 14) #dt(2022, 2, 1)\n",
    "args.end_date =  dt(2023, 2, 12) #dt(2024, 1, 31)\n",
    "args.number_of_business_day = 260\n",
    "use_GAN = False\n",
    "test_alert = load_SVM_pred(use_GAN, args)\n",
    "test_annomaly = load_test_annomaly(args)\n",
    "IAD = IncidentDetectionAnalysis(args, test_alert, test_annomaly)\n",
    "DR, FAR, MTTD, all_fail_to_detect_cases, all_false_postive_cases, Detected_Report_Dict, Detected_Report_Alert_Dict = IAD.check_overall_results()\n",
    "print('DR', DR, 'FAR', FAR, 'MTTD', MTTD)\n",
    "SDR, SFAR, SMTTD, Sall_fail_to_detect_cases, Sall_false_postive_cases, SDetected_Report_Dict, SDetected_Report_Alert_Dict = IAD.check_overall_s_results()\n",
    "print('SDR', SDR, 'SFAR', SFAR, 'SMTTD', SMTTD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "day_number_is 26\n",
      "detected!!!!!!! [-7.566666666666666] -7.566666666666666\n",
      "detected!!!!!!! [11.816666666666666] 11.816666666666666\n",
      "detected!!!!!!! [-45.13333333333333] -45.13333333333333\n",
      "DR 0.42857142857142855 FAR 0.6 MTTD -13.627777777777775\n",
      "SDR 0.75 SFAR 0.2 SMTTD -13.627777777777775\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\haowu\\AppData\\Local\\Temp\\ipykernel_293732\\3925642866.py:19: FutureWarning: In a future version of pandas, parsing datetimes with mixed time zones will raise an error unless `utc=True`. Please specify `utc=True` to opt in to the new behaviour and silence this warning. To create a `Series` with mixed offsets and `object` dtype, please use `apply` and `datetime.datetime.strptime`\n",
      "  df_waze_link['Start time'] = pd.to_datetime(df_waze_link['Start time'], errors='coerce')\n",
      "c:\\Users\\haowu\\Anaconda3\\envs\\traffic\\lib\\site-packages\\geopandas\\geodataframe.py:1443: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  super().__setitem__(key, value)\n",
      "C:\\Users\\haowu\\AppData\\Local\\Temp\\ipykernel_293732\\3925642866.py:20: FutureWarning: In a future version of pandas, parsing datetimes with mixed time zones will raise an error unless `utc=True`. Please specify `utc=True` to opt in to the new behaviour and silence this warning. To create a `Series` with mixed offsets and `object` dtype, please use `apply` and `datetime.datetime.strptime`\n",
      "  df_waze_link['Closed time'] = pd.to_datetime(df_waze_link['Closed time'], errors='coerce')\n",
      "c:\\Users\\haowu\\Anaconda3\\envs\\traffic\\lib\\site-packages\\geopandas\\geodataframe.py:1443: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  super().__setitem__(key, value)\n"
     ]
    }
   ],
   "source": [
    "args.link_id = '110+04483'\n",
    "args.county = \"TSMO\"\n",
    "args.model_path = 'E:/two_stage_model'\n",
    "args.start_date = dt(2022, 2, 14) #dt(2022, 2, 1)\n",
    "args.end_date =  dt(2023, 2, 12) #dt(2024, 1, 31)\n",
    "args.number_of_business_day = 260\n",
    "use_GAN = True\n",
    "test_alert = load_SVM_pred(use_GAN, args)\n",
    "test_annomaly = load_test_annomaly(args)\n",
    "IAD = IncidentDetectionAnalysis(args, test_alert, test_annomaly)\n",
    "DR, FAR, MTTD, all_fail_to_detect_cases, all_false_postive_cases, Detected_Report_Dict, Detected_Report_Alert_Dict = IAD.check_overall_results()\n",
    "print('DR', DR, 'FAR', FAR, 'MTTD', MTTD)\n",
    "SDR, SFAR, SMTTD, Sall_fail_to_detect_cases, Sall_false_postive_cases, SDetected_Report_Dict, SDetected_Report_Alert_Dict = IAD.check_overall_s_results()\n",
    "print('SDR', SDR, 'SFAR', SFAR, 'SMTTD', SMTTD)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  },
  "vscode": {
   "interpreter": {
    "hash": "4c7976fd35f11d6b5acd37b5df05c2b1d9460872f4ccb0abe3e95e9a1eb343e6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
